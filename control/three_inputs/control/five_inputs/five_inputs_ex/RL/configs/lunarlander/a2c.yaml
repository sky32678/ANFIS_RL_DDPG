experiment_info:
  # Main algorithm choices
  experiment_name: A2C
  agent: rlcycle.a2c.agent.A2CAgent
  learner: rlcycle.a2c.learner.A2CLearner
  critic_loss: rlcycle.a2c.loss.DiscreteCriticLoss
  actor_loss: rlcycle.a2c.loss.DiscreteActorLoss 
  action_selector: rlcycle.a2c.action_selector.A2CDiscreteActionSelector
  worker_device: cpu
  device: cpu
  log_wandb: True
  
  # Environment info
  env:
    name: "LunarLander-v2"
    is_atari: False
    is_discrete: True
    max_episode_steps: 300

  # Experiment default arguments:
  total_num_episodes: 2000
  max_update_steps: 3000
  test_interval: 100  # Test every 50 episodes
  test_num: 5  # Number of episodes to test during test phase
  train_render: False  # Render all episode steps
  test_render: True # Render test
  num_workers: 2

defaults:
  - hyper_params: a2c_mnih2016
  - models: a2c