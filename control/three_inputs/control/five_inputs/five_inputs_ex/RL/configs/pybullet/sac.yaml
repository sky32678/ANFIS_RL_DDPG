experiment_info:
  # Main algorithm choices
  experiment_name: SAC-Agent
  agent: rlcycle.sac.agent.SACAgent
  learner: rlcycle.sac.learner.SACLearner
  critic_loss: rlcycle.sac.loss.CriticLoss
  actor_loss: rlcycle.sac.loss.PolicyLoss
  action_selector: rlcycle.sac.action_selector.SACActionSelector
  device: cuda
  log_wandb: True

  # Environment info
  env:
    name: ReacherPyBulletEnv-v0
    is_atari: False
    is_bullet: True

  # Experiment default arguments:
  total_num_episodes: 50000
  test_interval: 500  # Test every 50 episodes
  test_num: 10  # Number of episodes to test during test phase
  render_train: True  
  render_test: True


defaults:
 - hyper_params: sac_haarnoja2019
 - models: sac
